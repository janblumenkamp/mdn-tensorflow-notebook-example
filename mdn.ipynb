{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDN Hands On Tutorial\n",
    "This notebook demonstrates the construction of a simple MDN, and compares it to a regular neural network.\n",
    "\n",
    "Read about MDNs on the [original paper](https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf) by C. Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network we'll construct will try to learn the following relation between $x$ and $f(x)$: \n",
    "\n",
    "$$f(x) = x^2-6x+9$$\n",
    "\n",
    "\n",
    "Note that this simply $y = x^2$ shifted three steps to the left (global minimum is at $x=3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2-6*x+9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the data a little bit more relaistic, we'll add a normally-distributed noise, which will be location-dependent - the larger $x$ is, the larger the noisier the data will be. So, our data generator will obey the following relation:\n",
    "\n",
    "$$g(x) = f(x) + \\epsilon(x) $$ \n",
    "\n",
    "$$ \\text{where}: \\epsilon(x) = N(0,\\sigma_0 x)$$\n",
    "\n",
    "Where $N(\\mu,\\sigma)$ is the normal distribution with mean $\\mu$ and STD of $\\sigma$.\n",
    "\n",
    "The `data_generator` below function creates $n$ nosiy data samples for a given `x`, where $n$ is defined by `samples`. Notice that technically, `data_generator` yields $g(x) = N(f(x),\\sigma_0 x)$, as mathematically that's the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x,sigma_0,samples):\n",
    "    return np.random.normal(f(x),sigma_0*x,samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now generate our dataset for $1<x<5$.\n",
    "\n",
    "The purple line in the plot presents the \"clean\" function $f(x)$ for this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_0 = 0.1\n",
    "x_vals = np.arange(1,5.2,0.2)\n",
    "x_arr = np.array([])\n",
    "y_arr = np.array([])\n",
    "samples = 50\n",
    "for x in x_vals:\n",
    "    x_arr = np.append(x_arr, np.full(samples,x))\n",
    "    y_arr = np.append(y_arr, data_generator(x,sigma_0,samples))\n",
    "x_arr, y_arr = shuffle(x_arr, y_arr)\n",
    "x_test = np.arange(1.1,5.1,0.2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('g(x)')\n",
    "ax.scatter(x_arr,y_arr,label='sampled data')\n",
    "ax.plot(x_vals,list(map(f,x_vals)),c='m',label='f(x)')\n",
    "ax.legend(loc='upper center',fontsize='large',shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular neural network\n",
    "We'll now train a neural network which will receive $x$ as input and our noisy $g(x)$ but will have to learn the relation $x \\rightarrow f(x)$.\n",
    "\n",
    "The network is constructed of two hidden layers, each with 12 nodes and the $\\tanh(x)$ activation function (note we don't use any activation on the last output layer).\n",
    "\n",
    "We set the learning rate $\\alpha=0.0003$, 50 examples per mini-batch and a total of 500 epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 50\n",
    "learning_rate = 0.0003\n",
    "\n",
    "l_inp = tf.keras.layers.Input(1)\n",
    "l = l_inp\n",
    "for _ in range(3):\n",
    "    l = tf.keras.layers.Dense(12, activation=\"tanh\")(l)\n",
    "l_out = tf.keras.layers.Dense(1)(l)\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.losses.MSE(y_true, y_pred))\n",
    "    \n",
    "s = tf.keras.models.Model(inputs=[l_inp], outputs=[l_out])\n",
    "s.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss = loss)\n",
    "s.summary()\n",
    "\n",
    "s.fit(\n",
    "    x=x_arr,\n",
    "    y=y_arr,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "y_pred = s.predict(np.expand_dims(x_test,axis=1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "ax.scatter(x_arr,y_arr,c='b',label='sampled data')\n",
    "ax.scatter(x_test,y_pred,c='r',label='predicted values')\n",
    "ax.plot(x_vals,list(map(f,x_vals)),c='m',label='f(x)')\n",
    "ax.legend(loc='upper center',fontsize='large',shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be doing quite good in predicting $f(x)$, but we can clearly see that the network learnt nothing about the size of the noise. \n",
    "\n",
    "## Mixture density network (MDN)\n",
    "Let's try an MDN now. We'll use the same network as in the previous section, with one important change:\n",
    "the output layer now has two nodes (which are constructed as two layers of 1 node for technical simplicity), which we named `mu` and `sigma`\n",
    "\n",
    "Note the new cost function: we create a normal distribution out of the predicted `mu` and `sigma`, and then minimize the negative log-likelihood of this distribution yielding the traget value `y`. Mathematically, our cost function is the negative logarithm of the normal distribution's probability density function (PDF):\n",
    "\n",
    "$$Cost = -\\log (PDF) = -\\log\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\cdot\\exp{\\left[-\\frac{(y-\\mu)^{2}}{2\\sigma^{2}}\\right]}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_cost(y_true, y_pred):\n",
    "    dist = tfp.distributions.Normal(loc=tf.expand_dims(y_pred[:,0], 1), scale=tf.expand_dims(y_pred[:,1], 1))\n",
    "    return tf.reduce_mean(-dist.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `elu + 1` as the activation function for `sigma`, as it must always be non-negative. The Exponential Linear Unit (ELU) is defined as:\n",
    "\n",
    "$$ ELU(x) = \\begin{cases} x & x\\ge0 \\\\ \\exp{(x)}-1 & x < 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_inp = tf.keras.layers.Input(1)\n",
    "l = l_inp\n",
    "for _ in range(3):\n",
    "    l = tf.keras.layers.Dense(12, activation=\"tanh\")(l)\n",
    "l_out_mu = tf.keras.layers.Dense(1)(l)\n",
    "l_out_sigma = tf.keras.layers.Dense(1, activation=lambda x: tf.nn.elu(x) + 1)(l)\n",
    "l_out = tf.keras.layers.concatenate(inputs=[l_out_mu, l_out_sigma], axis=1)\n",
    "\n",
    "s = tf.keras.models.Model(inputs=[l_inp], outputs=[l_out])\n",
    "s.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss=mdn_cost)\n",
    "s.summary()\n",
    "\n",
    "s.fit(\n",
    "    x=x_arr,\n",
    "    y=y_arr,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "y_pred = s.predict(np.expand_dims(x_test,axis=1))\n",
    "mu_pred = y_pred[...,0]\n",
    "sigma_pred = y_pred[...,1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "ax.errorbar(x_test,mu_pred,yerr=np.absolute(sigma_pred),c='r',ls='None',marker='.',ms=10,label='predicted distributions')\n",
    "ax.scatter(x_arr,y_arr,c='b',alpha=0.05,label='sampled data')\n",
    "ax.errorbar(x_vals,list(map(f,x_vals)),yerr=list(map(lambda x: sigma_0*x,x_vals)),c='b',lw=2,ls='None',marker='.',ms=10,label='true distributions')\n",
    "ax.plot(x_vals,list(map(f,x_vals)),c='m',label='f(x)')\n",
    "ax.legend(loc='upper center',fontsize='large',shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the results learnt by the network. In red are the networks predictions for $\\mu$ and $\\sigma$, and in blue are the actual $\\mu$ and $\\sigma$ used for the training set. The actual data can be seen faded in the background. We can clearly see the network has learnt not just $x \\rightarrow f(x)$, but also the noise creating $x \\rightarrow g(x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
